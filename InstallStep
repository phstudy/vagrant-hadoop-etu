[Prerequistive - JDK]
$ sudo apt-get install openjdk-7-jdk
$ java –version

[Prerequistive - SSH]
$ sudo apt-get install openssh-server

[Hadoop]
(Add Hadoop Group and User)
$ sudo addgroup hadoop
$ sudo adduser –ingroup hadoop hduser
$ sudo adduser hduser sudo
(After user is created, re-login into Ubuntu using hduser)

(Set SSH Certificate)
$ ssh-keygen –t rsa –P ‘’
$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
$ ssh localhost

(Download Hadoop 2.4.0)
$ cd ~
$ wget http://www.trieuvan.com/apache/hadoop/common/hadoop-2.4.0/hadoop-2.4.0.tar.gz
$ sudo tar vxzf hadoop-2.4.0.tar.gz –C /usr/local
$ cd /usr/local
$ sudo mv hadoop-2.4.0 hadoop
$ sudo chown –R hduser:hadoop hadoop

(Set Hadoop Environment Variables)
$ cd ~
$ nano .bashrc

Paste following to the end of the file
#Hadoop variables
export JAVA_HOME=/usr/lib/jvm/jdk/
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
###End of paste

$ cd /usr/local/hadoop/etc/hadoop
$ nano hadoop-env.sh

#modify JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/jdk/

(Re-login into Ubuntu using hduser and check the hadoop version)
$ hadoop version

(Configure Hadoop)
$ cd /usr/local/hadoop/etc/hadoop
$ nano core-site.xml

#paste following between <configuration>
fs.default.name
hdfs://localhost:9000

$ nano yarn-site.xml

#paste following between <configuration>
yarn-nodemanager.aux-services
mapreduce_shuffle

yarn-nodemanager.aux-services.mapreduce.shuffle.class
org.apache.hadoop.mapred.ShuffleHandler

$ mv mapred-site.xml.template mapred-site.xml
$ nano mapred-site.xml

#paste following between <configuration>
mapreduce.framework.name
yarn

$ cd ~
$ mkdir –p mydata/hdfs/namenode
$ mkdir –p mydata/hdfs/datanode
$ cd /usr/local/hadoop/etc/hadoop
$ nano hdfs-site.xml


#paste following between <configuration>

dfs.replication
1

dfs.namenode.name.dir
file:/home/hduser/mydata/hdfs/namenode

dfs.datanode.data.dir
file:/home/hduser/mydata/hdfs/datanode

(Format Namenode)
$ hdfs namenode -format

(Start Hadoop Service)
$ start-dfs.sh
$ start-yarn.sh
$ jps

(Run Hadoop Example)
$ cd /usr/local/hadoop
$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.0.jar pi 2 5

[ZooKeeper]
TBA

[HBase]
http://archive.cloudera.com/cdh/3/hbase-0.90.1-cdh3u0/quickstart.html#d0e89

(Download HBase http://archive.apache.org/dist/hbase/stable/hbase-0.94.18.tar.gz)
$ tar –xvf hbase-0.94.18.tar.gz
$ sudo mkdir /home/lu/programs/hbase
$ mv hbase-0.94.18 /home/lu/programs/hbase

(Configure HBase with Java)
$ cd /home/lu/programs/hbase/conf
$ nano hbase-env.sh

export JAVA_HOME=/home/lu/programs/java/….

$ cd ~
$ gedit ~/.bashrc

export HBASE_HOME=/home/lu/programs/hbase
export PATH=$PATH:$HBASE_HOME/bin

(Change the directory to where you want to HBase to write to)
$ cd /home/lu/programs/hbase/conf
$ nano hbase-site.xml

hbase.rootdir = ……

(Extra steps)
In /etc/hosts there are two entries: 127.0.0.1 and 127.0.1.1, change the second entry 127.0.1.1 to 127.0.0.1 otherwise it gives error: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing

(Start HBase)
HBASE_PATH$ bin/start-hbase.sh
HBASE_PATH$ bin/hbase shell

(Stop HBase)
HBASE_PATH$ bin/stop-hbase.sh

(Use the web interfaces)
For master: http://localhost:60010
For region server: http://localhost:60030

[Pig]
https://pig.apache.org/docs/r0.7.0/setup.html
(Prerequistive: Hadoop, Java, Ant, JUnit)

(Download Pig and unpack it)
http://pig.apache.org/releases.html#Download
($ export PATH=/<PigPath>/pig-n.n.n/bin:$PATH)
($ pig –help)
($ pig)
http://archanaschangale.wordpress.com/2013/10/14/pig-installation-on-ubuntu/comment-page-1/

$ tar –xvf pig-n.n.n.tar.gz
$ sudo mkdir /home/lu/programs/pig
$ mv pig-n.n.n /home/lu/programs/ pig
$ cd ~
$ gedit ~/.bashrc

export PIG_HOME=/home/lu/programs/pig
export PATH=$PATH:$PIG_HOME/bin

(Restart the computer)

(Test the installation)
$ pig -h

(Start pig in local mode)
$ pig –x local grunt>

(Start pig in mapreduce mode)
$ pig –x mapreduce  (or $ pig)


[Hive]
https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-Requirements
(Download and Install Hive)
$ tar –xzvf hive-0.13.0.tar.gz
$ cd /home/lu/programs/hive
$ export HIVE_HOME={{pwd}}
$ export PATH=$HIVE_HOME/bin:$PATH

(Build Hive)
$ svn co http://svn.apache.org/repos/asf/hive/trunk hive
$ cd hive
$ mvn clean install –Phadoop-2, dist
$ cd packaging/target/apache-hive-{version}-SNAPSHOT-bin/apache-hive-{version}-SNAPSHOT-bin
$ ls

(Run Hive)
$ $HADOOP_HOME/bin/hadoop fs –mkdir       /tmp
$ $HADOOP_HOME/bin/hadoop fs –mkdir       /user/hive/warehouse
$ $HADOOP_HOME/bin/hadoop fs –chmod g+w  /tmp
$ $HADOOP_HOME/bin/hadoop fs –chmod g+w  /user/hive/warehouse
$ export HIVE_HOME=/home/lu/programs/hive
$ $ HIVE_HOME/bin/hive


[HDFS Web Interface]
http://<NameNodeIP>:50070 (by default)
